_target_: TVB.workspace.train_vqvae_representation_workspace.TrainVqvaeRepresentationWorkspace
base_learning_rate: 4.5e-6

model:
  embed_dim: 3
  n_embed: 1024
  ddconfig:
    double_z: False
    z_channels: 3
    resolution: 256
    in_channels: 3
    out_ch: 3
    ch: 128
    ch_mult: [1,1,2,4] 
    num_res_blocks: 2
    attn_resolutions: [16]
    dropout: 0.0
  lossconfig:
    target: UniT.taming.modules.losses.vqperceptual.VQLPIPSWithDiscriminator
    params:
      disc_conditional: False
      disc_in_channels: 3
      disc_start: 2000
      disc_weight: 0.8
      codebook_weight: 1.0

shape_meta: &shape_meta
  # acceptable types: rgb, low_dim
  obs:
    left_tactile_camera_taxim:
      shape: [3, 256, 256]
      type: rgb
  # action:
  #   shape: [6]


dataset:
  shape_meta: ${shape_meta}
  zarr_path: data/unit_train_dataset
  horizon: 1
  pad_before: 1
  pad_after: 1
  seed: 42
  val_ratio: 0.00
  max_train_episodes: null
val_dataloader:
  batch_size: 8
  num_workers: 8
  persistent_workers: false
  pin_memory: true
  shuffle: false
dataloader:
  batch_size: 8
  num_workers: 8
  persistent_workers: false
  pin_memory: true
  shuffle: true
gpus: '1,'
batch_size: 16
seed: 42
trainer:
  name: "vq_representiaon_dubug"
  resume: ""
  base: []
  no_test: false
  project: null
  debug: false
  seed: 42
  postfix: ""
  train: true
save_every: 1
max_epochs: 50